code forces 1

#include<iostream>
#include<vector>
#include<unistd.h>
#include<string>
#include<sstream>
#include<iterator>
#include<assert.h>
using namespace std;

int main()
{
        int T;
        cin>>T;
        vector<int>result;
        for(int i=0;i<T;++i)
        {
                vector<int> arr;
                int n;
                cin>>n;
                int prev=-1,curr;
                bool sorted=true;
                for(int j=0;j<n;++j)
                {
                        cin>>curr;
                        if(curr<prev)
                                sorted=false;
                        prev=curr;
                        arr.push_back(curr);
                }
                if(sorted)
                {
                        result.push_back(n);
                        continue;
                }

                int l_result=0;
                while(!sorted)
                {
                        if(n==1)
                        {
                                l_result = 1;
                                break;
                        }
                        n=n/2;
                        assert(n!=0);
                        int n_loops=arr.size()/n;

                        for(int j=0;j<n_loops;++j)
                        {
                                int start_index=j*2;
                                int stop_index=(j+1)*2;
                                prev=-1;
                                for(int k=start_index;k<stop_index;++k)
                                {
                                        if(arr[k]<prev)
                                        {
                                                sorted = false;
                                                break;
                                        }
                                        else
                                                sorted = true;
                                        prev=arr[k];
                                }
                                if(sorted)
                                        l_result = n;

                        }
                }
                result.push_back(l_result);

        }

        for(auto&x:result)
                cout<<endl<<x<<endl;

        return 0;
}





techgig 2
#include<iostream>
#include<vector>
#include<map>
using namespace std;

int64_t maxSum(int64_t a,int64_t b)
{
return (a>b?a:b);
}

int main()
{
        int64_t T;
        cin>>T;
        vector<string>result;

        for(int t=0;t<T;++t)
        {
                int64_t N;
                cin>>N;

                map<int,vector<int> >included_map,excluded_map;
                vector<int64_t>included_sum,excluded_sum;

                bool firstNonZeroFlag=false;
                vector<int>ticket;
                for(int n=0;n<N;++n)
                {
                        int temp;
                        cin>>temp;

                        //This ensures that our vector's first element will be non zero
                        if(firstNonZeroFlag==false && temp !=0)
                        {
                                firstNonZeroFlag=true;
                        }

                        if(firstNonZeroFlag)
                                ticket.push_back(temp);
                }

                //If all inputs are zero(added for extra safety although not required)
                if(firstNonZeroFlag==false)
                {
                        result.push_back(to_string(0));
                        continue;
                }

                included_sum.push_back(ticket[0]);
                excluded_sum.push_back(-1001);
                {
                        vector<int>temp;
                        temp.push_back(ticket[0]);
                        included_map[0]=temp;
                }
                for(int n=1;n<N;++n)
                {
                        int64_t includedSum,excludedSum;

                        if(ticket[n]!=0)
                        {
                                includedSum=maxSum(ticket[n], ticket[n]+excluded_sum[n-1]);
                                included_sum.push_back(includedSum);
                                if(includedSum==ticket[n])
                                {
                                        vector<int> temp;
                                        temp.push_back(ticket[n]);
                                        included_map[n]=temp;
                                }
                                else
                                {
                                        vector<int> temp;
                                        temp=excluded_map[n-1];
                                        temp.push_back(ticket[n]);
                                        included_map[n]=temp;
                                }

                                excludedSum=maxSum(excluded_sum[n-1],included_sum[n-1]);
                                excluded_sum.push_back(excludedSum);

                                if(excludedSum==excluded_sum[n-1])
                                        excluded_map[n]=excluded_map[n-1];

                                else
                                        excluded_map[n]=included_map[n-1];

                        }

                        else
                        {
                                excludedSum=maxSum(excluded_sum[n-1],included_sum[n-1]);
                                excluded_sum.push_back(excludedSum);

                                if(excludedSum==excluded_sum[n-1])
                                        excluded_map[n]=excluded_map[n-1];

                                else
                                        excluded_map[n]=included_map[n-1];

                                included_sum.push_back(included_sum[n-1]);
                                included_map[n]=included_map[n-1];
                        }

                        cout<<endl<<"n = "<<n<<endl;
                        cout<<endl<<"printing included map"<<endl;
                        for(auto&x : included_map[n])
                                cout<<x;
                        cout<<endl<<"printing excluded map"<<endl;
                        for(auto&x : excluded_map[n])
                                cout<<x;
                }
                //case 1 included_sum[N-1] > excluded_sum[N-1]
                if(included_sum[N-1] > excluded_sum[N-1])
                {
                        printf("\n[%s::%d] case 1 included_sum[N-1] > excluded_sum[N-1]\n",__FILE__,__LINE__);
                        string temp;
                        for(auto&x:included_sum)
                                temp=to_string(x)+temp;

                        result.push_back(temp);
                }

                //case 2 included_sum[N-1] < excluded_sum[N-1]
                else if(included_sum[N-1] < excluded_sum[N-1])
                {
                        printf("\n[%s::%d] case 2 included_sum[N-1] < excluded_sum[N-1]\n",__FILE__,__LINE__);
                        string temp;
                        for(auto&x:excluded_sum)
                                temp=to_string(x)+temp;

                        result.push_back(temp);

                }

                //case 3 included_sum[N-1] == excluded_sum[N-1]
                else
                {
                        printf("\n[%s::%d] case 3 included_sum[N-1] == excluded_sum[N-1]\n",__FILE__,__LINE__);
                        int i,j;

                        vector<int>& inc=included_map[N-1];
                        vector<int>& exc=excluded_map[N-1];
                        for(i=inc.size()-1, j= exc.size()-1;i>=0;--i)
                        {
                                //case 1
                                if(inc[i]>exc[j])
                                {
                                        printf("\n[%s::%d] case 1\n",__FILE__,__LINE__);
                                        string temp;
                                        for(auto&x:inc)
                                                temp=to_string(x)+temp;

                                        result.push_back(temp);

                                        break;
                                }
                                //case 2
                                else if(inc[i]<exc[j])
                                {
                                        printf("\n[%s::%d] case 2\n",__FILE__,__LINE__);
                                        string temp;
                                        for(auto&x:exc)
                                                temp=to_string(x)+temp;

                                        result.push_back(temp);

                                        break;
                                }
                                //case 3
                                else
                                {
                                        printf("\n[%s::%d] case 3\n",__FILE__,__LINE__);
                                        --j;
                                        continue;
                                }


                        }
                }
        }
        for(auto&x: result)
                cout<<x<<endl;

        return 0;
}




techgig 1
#include<iostream>
#include<vector>
#include<algorithm>
using std::cout;
using std::vector;
using std::string;
using std::cin;
using std::endl;
int main()
{
        int t,n;
vector<string> win_flag;
vector<int>s_v,s_p;
        cin>>t;
        for(int i=0;i<t;i++)
        {
                cin>>n;
                for(int j=0;j<n;j++)
                {
                        int temp;
                        cin>>temp;
                        s_v.push_back(temp);
                }
                for(int j=0;j<n;j++)
                {
                        int temp;
                        cin>>temp;
                        s_p.push_back(temp);

                }
                sort(s_v.begin(), s_v.end());
                sort(s_p.begin(), s_p.end());
                int l_count=0;
                for(int i=0;i<n;i++)
                {
                        if(s_v[i]>s_p[i])
                        {
                                l_count=-1;
                                win_flag.push_back("LOSE");
                                break;
                        }
                }
                if(l_count==0)
                        win_flag.push_back("WIN");
        }
for(auto&x:win_flag)
cout<<x<<endl;
        return 0;
}





HIREDIS SAMPLE CODE

#include <stdio.h>

#include <stdlib.h>

#include <string.h>

#include<unistd.h>

#include <hircluster.h>

#include<iostream>

using namespace std;


int main(int argc, char **argv) 
{


struct timeval timeout = { 1, 500000 }; // 1.5 seconds

redisClusterContext *cc = redisClusterContextInit();

redisClusterSetOptionAddNodes(cc, "172.16.129.68:6380");

redisClusterSetOptionConnectTimeout(cc, timeout);

redisClusterSetOptionRouteUseSlots(cc);

redisClusterConnect2(cc);

if (cc != NULL && cc->err) 
{
    
printf("Error: %s\n", cc->errstr);
    // handle error

exit(-1);

}




/*

//To print all the master nodes

print_cluster_node_list(cc);

*/




/*

//Finding hash slot of the key

unsigned int key_slot=keyHashSlot("temp", 4);

cout<<"Key slot for temp: "<<key_slot<<endl;
    

redisReply *reply;
	
reply = (redisReply*)(redisClusterCommand(cc,"cluster nodes"));
	
if (cc != NULL && cc->err) 
		
printf("Error: %s\n", cc->errstr);
	
else
		
printf("cluster nodes: %s\n", reply->str);
	
freeReplyObject(reply);
*/



/*

// Set a hash map
    
redisReply *reply;
	
reply = (redisReply*)(redisClusterCommand(cc,"hmset test_hash bin1_name bin1_value bin2_name bin2_value bin3_name bin3_val"));
	
if (cc != NULL && cc->err) 
		
printf("Error: %s\n", cc->errstr);
	
else
		
printf("Status: %s\n", reply->str);
	
freeReplyObject(reply);

*/



/*

// Get the complete hash map
    
redisReply *reply;
	
reply = (redisReply*)(redisClusterCommand(cc,"hgetall test_hash"));
	
if (cc != NULL && cc->err) 
		
printf("Error: %s\n", cc->errstr);
	
else

{
	
cout<<endl<<"reply type: "<<reply->type<<endl;
	
if (reply->type == REDIS_REPLY_ARRAY) 
{
		
for (int j = 0; j < reply->elements; j++) 
{
			
printf("%u) %s\n", j, reply->element[j]->str);
		
}
	
}

}
	
freeReplyObject(reply);

*/




//Get specific bins of hash map


redisReply *reply;

reply = (redisReply*)(redisClusterCommand(cc,"hmget test_hash bin1_name bin3_name"));

if (cc != NULL && cc->err)
	
printf("Error: %s\n", cc->errstr);
	
else

{
	
cout<<endl<<"reply type: "<<reply->type<<endl;
	
if (reply->type == REDIS_REPLY_ARRAY) 
{
		
for (int j = 0; j < reply->elements; j++) 
{
			
printf("%u) %s\n", j, reply->element[j]->str);
		
}
	
}

}

freeReplyObject(reply);





/*
    
// PING server //
    
while(1)

{
	
reply = (redisReply*)(redisClusterCommand(cc,"PING"));
	
if (cc != NULL && cc->err) 
		
printf("Error: %s\n", cc->errstr);
	
else
		
printf("PING: %s\n", reply->str);
	
freeReplyObject(reply);

}

*/


/*    
// GET key //
    
while(1)

{

sleep(3);
	
reply = (redisReply*)(redisClusterCommand(cc,"GET foo"));
	
if (cc != NULL && cc->err) 
		
printf("Error: %s\n", cc->errstr);
	
else
		
printf("GET foo: %s\n", reply->str);
	
freeReplyObject(reply);

}

*/


/*
    
// Set a key //
    
reply = (redisReply*)(redisClusterCommand(cc,"SET %s %s", "foo", "hello vishal"));

if(cc->err) 
    
printf("\n[%s::%d]Error: %s\n", __FILE__,__LINE__,cc->errstr);

else
    
printf("SET: %s\n", reply->str);
    
freeReplyObject(reply);

*/


/*
    
// Set a key using binary safe API //
    
reply = (redisReply*)redisClusterCommand(cc,"SET %b %b", "bar", (size_t) 3, "hello", (size_t) 5);
    
printf("SET (binary API): %s\n", reply->str);
    
freeReplyObject(reply);

    

// Try a GET and two INCR //
    
reply = (redisReply*)redisClusterCommand(cc,"GET foo");
    
printf("GET foo: %s\n", reply->str);
    
freeReplyObject(reply);

    
reply = (redisReply*)redisClusterCommand(cc,"INCR counter");
    
printf("INCR counter: %lld\n", reply->integer);
    
freeReplyObject(reply);
    
// again ... //
    
reply = (redisReply*)redisClusterCommand(cc,"INCR counter");
    
printf("INCR counter: %lld\n", reply->integer);
    
freeReplyObject(reply);

    
// Create a list of numbers, from 0 to 9 //
    
reply = (redisReply*)redisClusterCommand(cc,"DEL mylist");
    
freeReplyObject(reply);
    
for (j = 0; j < 10; j++) 
{
        
char buf[64];

        
snprintf(buf,64,"%d",j);
        
reply = (redisReply*)redisClusterCommand(cc,"LPUSH mylist element-%s", buf);

        freeReplyObject(reply);

    }

    
// Let's check what we have inside the list //
    
reply = (redisReply*)redisClusterCommand(cc,"LRANGE mylist 0 -1");
    
if (reply->type == REDIS_REPLY_ARRAY) 
{
        
for (j = 0; j < reply->elements; j++) 
{
            
printf("%u) %s\n", j, reply->element[j]->str);

        }

    }
    
freeReplyObject(reply);

*/
    
// Disconnects and frees the context //
    
redisClusterFree(cc);


    return 0;

}



ns1.cp-ht-9.webhostbox.net
ns2.cp-ht-9.webhostbox.net

OJV-359-30749(ticket no)

673 footprints niti khand 2

email account: admin@dsscokhla.com
password: Password@5

dsscokhla.com/cpanel 
username dsscoplb
password +@~M7^3+RrZk

database name: dsscoplb_sailing_club_db_2992018
user name: dsscoplb_cptnav
password: Password@9

wordpress admin user name:
dsscokhla_sailing_club_cptnav2992018
password: Password@cptnav2992018

Database Name: wp236(old) wp764(new)
Table prefix: wpgf_(old) wpkm_(new)

wordpress administrative url:  http://dsscokhla.com/wp/wp-admin/


Linux Commands

To find the resources consumed by each thread of a pid
top -p PID -H


To find port forwarding in linux gateway

Ps –ef | grep fnNl(or just fn)

To find info about cpu (lscpu)
To find bits of OS(getconf LONG_BIT or uname -a)

To monitor system metrics
dstat -lrvn 10

finding number of java applications running on linux
ps -ef | grep "[j]ava" | wc –l


finding pid of running java applications
ps aux | grep java

finding info from pid
ps -p 3339 -o comm=

To create a user account from a shell prompt:
1.	Open a shell prompt.
2.	If you are not logged in as root, type the command su - and enter the root password.
3.	Type useradd followed by a space and the username for the new account you are creating at the command line (for example, useradd jsmith). Press [Enter]. Often, usernames are variations on the user's name, such as jsmith for John Smith. User account names can be anything from the user's name, initials, or birthplace to something more creative.
4.	Type passwd followed by a space and the username again (for example, passwd jsmith).
5.	At the New password: prompt enter a password for the new user and press [Enter].
6.	At the Retype new password: prompt, enter the same password to confirm your selection.


Route –n command will tell about the default gateway of the subnet.




























http://cassandra.apache.org/doc/latest/operating/index.html

Datastax’s C++ driver mailing list
https://groups.google.com/a/lists.datastax.com/forum/#!newtopic/cpp-driver-user

To Increase CQL timeout
Edit cqlsh.py file in bin folder. Modify the DEFAULT_REQUEST_TIMEOUT_SECONDS.


Installing Cassandra
(open ports 7000 and 9160)
1.	Install latest stable JDK package from oracle website.(to check java version type “java –version”
Install the jdk rpm package by using below given command.
rpm -ivh jdk-8u45-linux-x64.rpm

2.	Download and extract apache-cassandra tar.gz file in a directory of your choice(download from “http://www-eu.apache.org/dist/cassandra/” or “http://cassandra.apache.org/download/”.
(to extract) tar -zxvf apache-cassandra-3.9-bin.tar.gz
3.	Next is to create necessary directories (for cassandra to store data)  and assign permissions on those directories.
mkdir /var/lib/cassandra/data
mkdir /var/log/cassandra
mkdir /var/lib/cassandra/commitlog
chown -R cassandra:cassandra /var/lib/cassandra/data
chown -R cassandra:cassandra /var/log/cassandra/
chown -R cassandra:cassandra /var/lib/cassandra/commitlog


Installing C++ driver
1.	Install rpm “libuv-1.13.1-1.el6.x86_64.rpm” by rpm –ivh libuv-1.13.1-1.el6.x86_64.rpm
2.	Install rpm –ivh libuv-devel-1.13.1-1.el6.x86_64.rpm
3.	Install rpm –ivh cassandra-cpp-driver-2.7.0-1.el6.x86_64.rpm
4.	Install rpm –ivh cassandra-cpp-driver-devel-2.7.0-1.el6.x86_64.rpm
The libraries will be installed at /usr/lib64 and the header file at /usr/include






To synchronize time between different servers
Vi /etc/ntp.conf
Add the following line
Server ip iburst
Replace ip in above command with the ip address of the server with which you want to sync time.
And then use following command
/etc/init.d/ntpd restart

https://www.rootusers.com/how-to-synchronize-time-in-linux-with-ntp-peers/
Taking backup of a node

To fully restore data, you must have a complete backup, which consists of the following:

• A snapshot at a point in time
• All incremental backups from the time you took the snapshot
• All commitlog segments since the time you took the last incremental
backup

Cassandra can only restore data from a snapshot when the table schema exists. If the schema does not exist and has not been backed up, you must recreate the schema.
First take backup of schema via 
./cqlsh 172.16.128.130 -e "DESC SCHEMA" > db_schema.cql

To recreate the schema, 
./cqlsh 172.16.128.130 -e "SOURCE '/home/cass/share/cassandra/data/hss_1/subscriber_master_table-abd04bf00c9711e8b214c75879db7963/snapshots/1518439698119/schema.cql'"
1.The nodetool clearsnapshot command removes all existing snapshot files from the snapshot directory of each keyspace. You should make it part of your back-up process to clear old snapshots before taking a new one.
./nodetool clearsnapshot

2. To take snapshot of all the keyspaces on a node:
./nodetool snapshot
The snapshot is created in data_directory/keyspace_name/table_name-UUID/snapshots/snapshot_name directory.

3. To use the snapshot, copy all the snapshot files into the data/keyspace/table-uuid folder
e.g. cp snapshots/1303701623423/* /home/cass/share/cassandra/data/hss_1/subscriber_master_table-abd04bf00c9711e8b214c75879db7963/
(here hss_1 is the keyspace and subscriber_master_table is the table name)

4. If incremental_backups was set as true in yaml, copy all the files in the data/keyspace/table-uuid/backup/ folder into data/keyspace/table-uuid/.

5. Run the nodetool refresh command, specifying the keyspace and table name.
./nodetool refresh hss_1 subscriber_master_table

6. Run nodetool repair at the end on all replicas.
The snapshots and incremental backup files are not automatically deleted, therefore you’ll have to manually delete the older backup files.

To restore a node from a snapshot and incremental backups:
1.	Shut down the node to be restored.
2.	Clear all files the /var/lib/cassandra/commitlog (by default).
3.	Clear all *.db files in <data_directory_location>/<keyspace_name>, but DO NOT delete the /snapshots and /backupssubdirectories.
4.	Locate the most recent snapshot folder in <data_directory_location>/<keyspace_name>/snapshots/<snapshot_name>, and copy its contents into <data_directory_location>/<keyspace_name>.
5.	If using incremental backups as well, copy all contents of <data_directory_location>/<keyspace_name>/backups into <data_directory_location>/<keyspace_name>.
6.	Restart the node, keeping in mind that a temporary burst of I/O activity will consume a large amount of CPU resources.


Restoring a snapshot into a new cluster
(When we have to recover data from a snapshot from some other cluster into some new cluster)

First we have to setup the new cluster with exact same settings as were there in the older cluster. Also we’ll have to use the same tokens for each of the nodes.
1.	In the yaml file, uncomment the ‘initial_token’ and specify the tokens you want the node to use. Also make sure all other configurations are same as before.
2.	Delete all the data from data/system folder i.e. rm –rf *
3.	Start the node.
4.	All the schemas from the old cluster must be reproduced in the new cluster.
5.	Stop the node.
6.	Copy all the files from the snapshot directory into the relevant table’s directory in the new cluster.
7.	Restart the node.

Adding a new node

Edit the topology files of all nodes. Start the new node and then run nodetool repair on all nodes after the new node has been added. Lastly, run nodetool cleanup.
https://stackoverflow.com/questions/24559907/cassandra-avoid-nodetool-cleanup

Replacing a dead node
In order to replace a dead node, start cassandra with the JVM startup flag -Dcassandra.replace_address_first_boot=<dead_node_ip>. Once this property is enabled the node starts in a hibernate state, during which all the other nodes will see this node to be down.
The replacing node will now start to bootstrap the data from the rest of the nodes in the cluster. The main difference between normal bootstrapping of a new node is that this new node will not accept any writes during this phase.
Once the bootstrapping is complete the node will be marked “UP”, we rely on the hinted handoff’s for making this node consistent (since we don’t accept writes since the start of the bootstrap).
Note
If the replacement process takes longer than max_hint_window_in_ms you MUST run repair to make the replaced node consistent again, since it missed ongoing writes during bootstrapping.



Insufficient user resource limit errors
https://docs.datastax.com/en/archived/cassandra/2.0/cassandra/troubleshooting/trblshootInsufficientResources_r.html
check current values by “ulimit –a”
change values(if any):
vim /etc/security/limits.conf

cass soft nofile 32768
cass hard nofile 32768
cass soft memlock unlimited
cass hard memlock unlimited
cass soft as unlimited
cass hard as unlimited





Decommissioning a node
Decommissioning a node will copy it’s data to other nodes. Use “./nodetool decommission”.

Changing rack/dc of a node

stop the node, change the rack/dc in the relevant configuration files and then restart the node with the flags: -Dcassandra.ignore_dc=true -Dcassandra.ignore_rack=true

Copying data from CQL

copy tablename to 'filepath.txt'
copy tablename from 'filepath.txt' with header=false 

							
							
Copying Partial data from one table to another

COPY keyspace.columnfamily1 (column1, column2,...) TO 'temp.csv';
COPY keyspace.columnfamily2 (column1, column2,...) FROM 'temp.csv';

https://stackoverflow.com/questions/21363046/how-to-select-data-from-a-table-and-insert-into-another-table#21378330

Using nodetool from remote server

On the Cassandra node, in the Cassandra-env.sh file, uncomment the following line:
JVM_OPTS="$JVM_OPTS -Djava.rmi.server.hostname=172.16.129.67"

“if [ "x$LOCAL_JMX" = "x" ]; then
    LOCAL_JMX=no
Fi”

if [ "$LOCAL_JMX" = "yes" ]; then
  JVM_OPTS="$JVM_OPTS -Dcassandra.jmx.local.port=$JMX_PORT -XX:+DisableExplicitGC"
else
  JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.port=$JMX_PORT"
  JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.rmi.port=$JMX_PORT"
  JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.ssl=false"
  JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=false"
  JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.password.file=/etc/cassandra/jmxremote.password"

On the remote server, use nodetool like following:
./nodetool –p 7199 –h 172.16.129.67 status

Set password on JMX port

1.	Edit below lines in Cassandra-env.sh file:
JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=true"
JVM_OPTS="$JVM_OPTS -Dcom.sun.management.jmxremote.password.file=path to jmxremote.password file"

2.	Copy the jmxremote.password.template from /<jre_install_dir>/lib/management/ to path given above and rename to jmxremote.password
3.	Change ownership to cassandra and permission.
4.	Edit jmxremote.password and add the user and password:
cassandra_test test_pwd
To find out the jre version being used by Cassandra first find out the PID of Cassandra and then
sudo lsof -p 27039 | grep jre

5.	Add cassandra_test with readwrite permission to /<jre_install_dir>/lib/management/jmxremote.access:
monitorRole readonly
Cassandra_test readwrite
controlRole readwrite \
create javax.management.monitor.*,javax.management.timer.* \
unregister
6. Restart Cassandra
7. Run nodetool with user and password:
nodetool -u Cassandra_test -pw test_pwd status


UDT
CREATE TYPE test.address (
    street text,
    city text,
    zip int,
    phone set<frozen<phone_numbers>>
);

CREATE TYPE test.phone_numbers (
    phone1 int,
    phone2 int
);
To use UDT in Table
CREATE TABLE test.udt (
    id timeuuid PRIMARY KEY,
    address frozen<address>,
    name text
)

To insert in table having UDT

insert into udt (id, address , name ) values(now(),{street:'niti1',city:'ghaziabad',zip:101021,phone:{{phone1:9818,phone2:4340}}},'random');

To update UDT in table

update udt set address ={street: 'niti1', city: 'ghaziabad', zip: 101021, phone: {{phone1: 949948, phone2: 4340}}} where id = 8d023440-ccce-11e6-ae6c-dfc3f02cf2fb;
Generating UUID/timeuuid
insert into stuff (uid, name) values(now(), 'my name');


Enabling G1GC 
https://medium.com/@mlowicki/move-cassandra-2-1-to-g1-garbage-collector-b9fb27365509
https://tobert.github.io/pages/als-cassandra-21-tuning-guide.html


Running Cassandra as service
Steps needed to run Cassandra as service. PFA Cassandra script file in the end.
configure sudo access to cass user 
1. vim visudo
2. ## Allows people in group wheel to run all commands
                %cass        ALL=(ALL)       ALL
3. sudo whoami (check if sudo access is given)

edit cassandra script and add to /etc/init.d/
1. configure CASS_HOME , CASS_BIN , CASS_LOG , CASS_USER , CASS_PID          in cassandra
2. chmod +x cassandra
3. sudo cp cassandra /etc/init.d/ 
4. chkconfig –-add cassandra
5. chkconfig cassandra on
6. sudu service cassandra status/start/stop

#!/bin/bash
# init script for Cassandra.
# chkconfig: 2345 90 10
# description: Cassandra
# script slightly modified from 
# http://blog.milford.io/2010/06/installing-apache-cassandra-on-centos/

. /etc/rc.d/init.d/functions

CASS_HOME=/cassandra/apache-cassandra-1.2.4
CASS_BIN=$CASS_HOME/bin/cassandra
CASS_LOG=$CASS_HOME/../log/system.log
CASS_USER="root"
CASS_PID=/var/run/cassandra.pid

if [ ! -f $CASS_BIN ]; then
  echo "File not found: $CASS_BIN"
  exit 1
fi

RETVAL=0

start() {
  if [ -f $CASS_PID ] && checkpid `cat $CASS_PID`; then
    echo "Cassandra is already running."
    exit 0
  fi
  echo -n $"Starting $prog: "
  daemon --user $CASS_USER $CASS_BIN -p $CASS_PID >> $CASS_LOG 2>&1
  usleep 500000
  RETVAL=$?
  if [ "$RETVAL" = "0" ]; then
    echo_success
  else
    echo_failure
  fi
  echo
  return $RETVAL
}

stop() {
  # check if the process is already stopped by seeing if the pid file exists.
  if [ ! -f $CASS_PID ]; then
    echo "Cassandra is already stopped."
    exit 0
  fi
  echo -n $"Stopping $prog: "
  if kill `cat $CASS_PID`; then
    RETVAL=0
    echo_success
 else
    RETVAL=1
    echo_failure
  fi
  echo
  [ $RETVAL = 0 ]
}

status_fn() {
  if [ -f $CASS_PID ] && checkpid `cat $CASS_PID`; then
    echo "Cassandra is running."
    exit 0
  else
   echo "Cassandra is stopped."
    exit 1
  fi
}

case "$1" in
  start)
    start
    ;;
  stop)
    stop
    ;;
  status)
    status_fn
    ;;
  restart)
    stop
  usleep 500000
    start
    ;;
  *)
    echo $"Usage: $prog {start|stop|restart|status}"
    RETVAL=3
esac

exit $RETVAL


Enabling JNA
Vim /etc/security/limits.conf 

cass soft nofile 32768
cass hard nofile 32768
cass soft memlock unlimited
cass hard memlock unlimited
cass soft nproc 32768
cass hard nproc 32768
cass hard as unlimited
cass soft as unlimited 

reboot


















Linux Procedures

To check whether the shell is login-shell or not

Echo $0
If bash then it’s a non login shell if –bash then it’s a login shell

The importance of using a login shell is the any settings in `/home/user/.bash_profile` will get executed. Here is a little more information if you are interested (from `man bash`)

When bash is invoked as an interactive login shell, or as a non-interactive shell with the --login option, it first reads and
executes commands from the file /etc/profile, if that file exists. After reading that file, it looks for ~/.bash_profile,
~/.bash_login, and ~/.profile, in that order, and reads and executes commands from the first one that exists and is readable.
The --noprofile option may be used when the shell is started to inhibit this behavior.


https://www.linuxquestions.org/questions/programming-9/how-to-check-in-a-script-whether-the-shell-is-login-or-non-login-360629/

Determining time difference between 2 servers


ntpdate -q host2

offset will give the time difference
https://superuser.com/questions/408753/determine-the-time-difference-between-two-linux-servers


To check the number of hard disks in the system

ls /dev/sd(tab)
the number of names without number is the count of the disks.
The name with numbers are the partitions.

To check which process is using a particular port
netstat -tulpn | grep 9042

To check communication between ports is possible

nc ip port_no //on server (ip is server ip)
nc –v -w2 ip port_no //on client(ip is server ip)


T
To open port

Iptables –I input –p tcp –m tcp –dport port_no –j ACCEPT

To check latency of tcp port

time nc –zw30 <host> <port>

to monitor tcp traffic on a port

tcpdump –A –i eth0 port 7000

Edit bashrc
Vim ~/.bashrc
*do changes*
:wq
Source ~/.bashrc


To  ssh into remote server without password
https://www.ssh.com/ssh/copy-id


find out about cpu info
lscpu and lscpu –e 


To find default gateway

Route –n
Look for the gateway having UG flag


Configuring multiple IP address to a single interface:



First find out which IP addresses are being used in your network..

nmap -sn -n 172.16.128.0/25
use the above command as root

if you don’t have root permission, check whether a given ip address is being used in your local network by doing following steps:

ping -c 1 -W 1 IP_Address
arp -a -n|grep -w IP_Address

https://serverfault.com/questions/912433/finding-unused-ip-addresses-on-my-network-in-linux/912454#912454

After that configure multiple virtual interfaces binded to the same physical interface
# cd /etc/sysconfig/network-scripts/
# cp ifcfg-eth0 ifcfg-eth0:0
# cp ifcfg-eth0 ifcfg-eth0:1
# cp ifcfg-eth0 ifcfg-eth0:2
ifcfg-eth0:0
DEVICE="eth0:0"
BOOTPROTO=static
ONBOOT=yes
TYPE="Ethernet"
IPADDR=172.16.16.126
NETMASK=255.255.255.224
GATEWAY=172.16.16.100
HWADDR=00:0C:29:28:FD:4C

Once, you’ve made all changes, save all your changes and restart/start the network service for the changes to reflect.
[root@tecmint network-scripts]# /etc/init.d/network restart
https://www.tecmint.com/create-multiple-ip-addresses-to-one-single-network-interface/




















Cassandra Info


PORTS in Cassandra

https://docs.datastax.com/en/cassandra/2.1/cassandra/security/secureFireWall_r.html

There are three primary ports of interest to Cassandra: 7000 (or 7001 if SSL/TLS is enabled), 7199, and 9160. Port 7000/7001 is used by Cassandra for cluster communication. This includes things such as the Gossip protocol and failure detection. Port 7199 is used by JMX. Port 9160 is the Thrift port and is used for client communication. In order for your cluster to function properly, all of these ports should be accessible.

Latest

http://cassandra.apache.org/doc/latest/faq/index.html#what-ports

By default, Cassandra uses 7000 for cluster communication (7001 if SSL is enabled), 9042 for native protocol clients, and 7199 for JMX. The internode communication and native protocol ports are configurable in the Cassandra Configuration File. The JMX port is configurable in cassandra-env.sh (through JVM options). All ports are TCP.

To check if a given port of a given IP is open for external communication
nc –vn IP Port_No

Copy command in CQL

https://www.datastax.com/dev/blog/new-features-in-cqlsh-copy

https://stackoverflow.com/a/41465577/5701173


Cassandra Internals
https://wiki.apache.org/cassandra/ArchitectureInternals



















Ignite

Installing autoconf

wget http://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz
$ tar xvfvz autoconf-2.69.tar.gz
$ cd autoconf-2.69
$ ./configure
$ make
$ sudo make install

After installation, verify autoconf version:
$ autoconf --version


Ignite C++
https://apacheignite-cpp.readme.io/docs/getting-started-1


https://cwiki.apache.org/confluence/display/IGNITE/Ignite+Durable+Memory+-+under+the+hood


Installing Ignite on 172.16.129.140

Copy  Ignite folder from 172.16.129.67 to 172.16.129.140
Copy *ignite* files from 172.16.129.67:/usr/local/lib to root@172.16.129.140: /usr/local/lib/
Copy the following lines in vim /home/ignite/.bashrc of 172.16.129.140

export LD_LIBRARY_PATH=/usr/local/lib:/usr/java/jdk1.8.0_111/jre/lib/amd64/server
export JAVA_HOME=/usr/java/jdk1.8.0_111
export IGNITE_HOME=/home/ignite/apache-ignite-fabric-2.4.0-bin

source ~/.bashrc

















Installing Python in RHEL

Download Python
Enter the following commands to download and extract Python 2.7 to your hosting account.
        mkdir ~/python
        
        cd ~/python
        
        wget http://www.python.org/ftp/python/2.7.2/Python-2.7.2.tgz
        
        tar zxfv Python-2.7.2.tgz
        
        find ~/python -type d | xargs chmod 0755
        
        cd Python-2.7.2
        
Install Python
Once extracted you can use the following commands to configure and install Python.
        ./configure --prefix=$HOME/python
        
        make
        
        make install
        
Modify the .bashrc
For your local version of python to load you will need to add it to the .bashrc file.
        vim ~/.bashrc
        
Press i 

Enter:
        export PATH=$HOME/python/Python-2.7.2/:$PATH
        
Write the changes (press ESC) and close vim:
        :wq
        
Press Enter
        source ~/.bashrc





 

systemd is the first daemon to start during booting and the last daemon to terminate during shutdown.

Unit files[edit]
systemd records initialization instructions for each daemon in a configuration file (referred to as a "unit file") that uses a declarative language, replacing the traditionally used per-daemon startup shell scripts. There are different types of unit files e.g. service units, target units, socket units, timer units (replacement for cron) etc. 
•	service
•	socket
•	device
•	mount
•	automount
•	swap
•	target
•	path
•	timer (which can be used as a cron-like job scheduler[16])
•	snapshot
•	slice (used to group and manage processes and resources[17])
•	scope

Unit file paths
1. /lib/systemd/system
For package-installed units

2. /etc/systemd/system
For administrator configured units

3. /run/systemd/system
For non persistent run time modifications

Unit files follow the naming convention:
name.type
e.g. ABC.service, SSH.socket etc


Each unit file has got 2 sections called [Unit] (contains description and dependencies) and [Install]

Service unit has got one extra section called [Service]
•	[Unit] — contains generic options that are not dependent on the type of the unit. These options provide unit description, specify the unit's behavior, and set dependencies to other units.
•	[unit type] — if a unit has type-specific directives, these are grouped under a section named after the unit type. 
•	[Install] — contains information about unit installation used by systemctl enable and disable commands.

Service units:
A unit configuration file whose name ends in .service encodes
information about a process controlled and supervised by systemd.
— systemd.service(5)
Systemd service units are the units that actually execute and keep track of programs and daemons, and dependencies are used to make sure that services are started in the right order. They are the most commonly used type of units.
Reference: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/sect-managing_services_with_systemd-unit_files

Target units:
A unit configuration file whose name ends in ".target" encodes
information about a target unit of systemd, which is used for grouping
units and as well-known synchronization points during start-up.
— systemd.target(5)
Targets are used for grouping and ordering units. They are somewhat of a rough equivalent to runlevels in that at different targets, different services, sockets, and other units are started. Unlike runlevels, they are much more free-form and you can easily make your own targets for ordering units, and targets have dependencies among themselves.
For instance, multi-user.target is what most daemons are grouped under, and it requires basic.target to be activated, which means that all services grouped under basic.target will be started before the ones in multi-user.target.

Slices/Control Groups
Systemd organizes processes in control groups. These are kernel level controllers that allow us to manage resource use on a system.  For example, all the processes started by an apache webserver will be in the same control group, CGI scripts included. This makes stopping an apache webserver much easier. This also moves the resource management settings from the process level to the application level by binding the system of CGroup hierarchies with the Systemd unit tree.

3 default cgroups: system, user and machine. Each of these cgroups is also known as a “slice”.

The three top level slices all have specific workloads that end up as sub-slices.
•	System is where daemons and services are placed.
•	User is where user sessions are placed. Each user gets a single slice under the main group, multiple logins using the same UID *do not* create more slices. This prevents clever nerds from stealing more resources than they are allowed.
•	Machine is where hosted virtual machines are placed, such as KVM guests.

With systemd it no longer necessary to create the cgroup manually. Instead, systemd introduces a new concept of slice, which is used to manage resources of a group of processes. Most services are placed in system.slice.

system processes get 1024 CPUShares, user processes 1024 CPUShares and virtual machines 1024 CPUShares, which means 33% of CPU each by default (you will only see it if each category executes some tasks). However, each of these can be configured.

One method of controlling resource usage is with the concept of “shares”. Shares are a relative value and the actual numbers only matter when compared to other values in the same cgroup. By default, slices are set to a value of 1024. So in our system slice above, httpd, sshd, crond and gdm are each set to use 1024 CPU shares. System, User and Machine are each set to 1024 themselves. If you’re having a hard time visualizing that, sometimes looking at it in the form of a tree helps:
•	System - 1024
o	httpd - 1024
o	sshd - 1024
o	crond - 1024
o	gdm - 1024
•	User - 1024
o	bash (mrichter) - 1024
o	bash (dorf) - 1024
•	Machine - 1024
o	testvm - 1024
In this case, we have a few daemons running, two users logged in (you’ll meet both of them later) and a single virtual machine. 

(reference: https://www.redhat.com/en/blog/world-domination-cgroups-part-1-cgroup-basics)
(reference: https://www.redhat.com/en/blog/world-domination-cgroups-part-2-turning-knobs)
(reference: https://www.certdepot.net/rhel7-get-started-cgroups/)


Every slice has multiple properties that can be set on it. As is always the case in Linux, we can create configuration files by hand or use the command line to set them.

Getting status of remote server

https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/sect-managing_services_with_systemd-remote



systemctl commands

Older Command	systemctl equivalent	Description
halt	systemctl halt	Halts the system
poweroff	systemctl poweroff	Powers off the system
reboot	systemctl reboot	Restarts the system
pm-suspend	systemctl suspend	Suspends the system
pm-hibernate	systemctl hibernate	Hibernates the system
pm-suspend-hybrid	systemctl hybrid-sleep	Hibernates and suspends the system


Suspend
– Suspending the system saves the system state in RAM and with the exception of the RAM module, powers off most of the devices in the machine.
– When you turn the machine back on, the system then restores its state from RAM without having to boot again.
– Because the system state is saved in RAM and not on the hard disk, restoring the system from suspend mode is significantly faster than restoring it from hibernation, but as a consequence, a suspended system state is also vulnerable to power outages.
# systemctl reload [name.service]
$ systemctl status [name.service]
# systemctl is-active [name.service]
$ systemctl list-units --type service –all

how long every program and service takes to start up
systemd-analyze blame

Listing running services
systemctl
to get the full hierarchy of control groups

systemd-cgls
To get the list of control group ordered by CPU, memory and disk I/O load, type:
systemd-cgtop

To allocate 70% of CPU to the system processes, 20% of CPU to the user processes and 10% of CPU to the virtual machines, type:
# systemctl set-property system.slice CPUShares=7168
# systemctl set-property user.slice CPUShares=2048
# systemctl set-property machine.slice CPUShares=1024
Note: Rebooting might be necessary to see the changes.
To restrict the user with the uid 1000 to use less than 20% of cpu, type:
# systemctl set-property user-1000.slice CPUQuota=20%
To reduce the memory available for the same user to 1GB, type:
# systemctl set-property user-1000.slice MemoryLimit=1024M
To limit the mariadb service to write below 2MB/s onto the /dev/vdb partition, type:
# systemctl set-property mariadb.service BlockIOWriteBandwidth="/dev/vdb 2M"
(reference: https://www.certdepot.net/rhel7-get-started-cgroups/)
To get the slice name of a particular service
systemctl show -p Slice foo.service

To change the CPU usage limit of a service
systemctl set-property foo.service CPUShares=250(The systemd.resource-control(5) man page has more information about cgroups-related parameters that you can pass to systemctl.)
Setting the property caused additional file 90-CPUShares.conf to be created under /etc/systemd/system/foo.service.d. This file contains parameters to be appended to the configuration of foo.service unit. Actually applying the change requires reloading systemd and restarting foo.service:
# systemctl daemon-reload
# systemctl restart foo.service
To cap a user’s(UID 1000) RAM
systemctl set-property user-1000.slice MemoryLimit=200M(200 MB is excluding swap space)

Restarting on exit
By default, systemd does not restart your service if the program exits for whatever reason. This is usually not what you want for a service that must be always available, so we’re instructing it to always restart on exit:
Restart=always
You could also use on-failure to only restart if the exit status is not 0.
By default, systemd attempts a restart after 100ms. You can specify the number of seconds to wait before attempting a restart, using:
RestartSec=1


****************************server-client-websocket*******************************************

g++ -std=c++17 -g -o client_exe WSClient.cpp -luWS -lssl -lz -lcrypto -lpthread

server.cpp

#include "../src/uWS.h"
#include<iostream>
using namespace std;
using namespace uWS;

int main() {
    Hub h;
    std::string response = "Hello!";

    h.onMessage([](WebSocket<SERVER> *ws, char *message, size_t length, OpCode opCode) {
        ws->send(message, length, opCode);
    });

    h.onHttpRequest([&](HttpResponse *res, HttpRequest req, char *data, size_t length,
                        size_t remainingBytes) {
        res->end(response.data(), response.length());
    });

    if (h.listen("172.16.129.67",6689)) {
            cout<<endl<<"listen success"<<endl;
        h.run();
    }
    else
    {
            cout<<endl<<"listen failed"<<endl;
            exit(-1);
    }
    while(1)
    {
    }
}


client.cpp

#include<iostream>
#include <stdio.h>
#include "../src/uWS.h"

using namespace std;
bool g_isConnected = false;

int main(int argc, char** argv)
{
 uWS::Hub h;
 uWS::WebSocket<uWS::CLIENT> *l_ws;

 h.onConnection([&l_ws](uWS::WebSocket<uWS::CLIENT> *ws, uWS::HttpRequest req) {
  printf("Inside onConnection....\n");
  g_isConnected = true;
 });

 h.onMessage([](uWS::WebSocket<uWS::CLIENT> *ws, char *message, size_t length, uWS::OpCode opCode) {
     printf("Message received from server: %s\n",message);
     printf("Message length received from server: %lu\n",length);
     printf("Message length (strlen)received from server: %lu\n",strlen(message));
     //  ws->send();
 });
 h.onDisconnection([&h](uWS::WebSocket<uWS::CLIENT> *ws, int code, char *message, size_t length) {
     printf("Server disconnected\n");
     g_isConnected = false;
 });


    h.onError([](void *user) {
      printf("\n[%s::%d]-->onError \n",__FILE__,__LINE__);
    });


 if(argc != 2)
 {
   printf("Usage: ./client <port>\n");
   exit(-1);
 }

 std::string l_port=(const char*)argv[1];
 std::string l_uri="ws://172.16.129.67:6689";
 printf("Connecting with URI: %s\n",l_uri.c_str());
 while(!g_isConnected)
 {
         cout<<endl<<"while loop begin"<<endl;
 h.connect(l_uri.c_str(), (void *) 34);
 h.run();
 sleep(2);
 cout<<endl<<"while loop ends"<<endl;
 }
 cout<<endl<<"Program ends"<<endl;
 return 0;
}




*****for JSON*****

JioDiaConfig.cpp

#include "pconfig/include/JioDiaConfig.hpp"

  namespace jio
  {
   namespace diameter
   {
    namespace pconfig
    {
>>   DiaConfig* DiaConfig::ms_instance = nullptr;

>>   DiaConfig* DiaConfig::getConfigInstance()
     {
      static DiaConfig * instance = new DiaConfig();
      return instance;
     }
>>   bool sort_asc_vect (uint32_t fa_first,uint32_t fa_second)
     {
      return (fa_first<fa_second);
     }

>>   int DiaConfig::initStackConfig(char* fa_plJsonDataFile)
     {
      printf("[%s:%d]--> Entry :: initStackConfig() \n", __FILE__,__LINE__);

      try{
       m_confFilePath=fa_plJsonDataFile;
       std::ifstream l_confDataIStream(fa_plJsonDataFile);
       std::string l_confDataIStreamStr((std::istreambuf_iterator<char>(l_confDataIStream)),(std::istreambuf_iterator<char>()));

       rapidjson::Document l_schemaDoc;
       m_confDataDoc.Parse(l_confDataIStreamStr.c_str());
       l_schemaDoc.Parse(g_plJsonSchema);

       if(l_schemaDoc.HasParseError())
       {
        //fprintf(stderr, "\nError(offset %u): %s\n", (unsigned)l_schemaDoc.GetErrorOffset(),GetParseError_En(l_schemaDoc.GetParseError()));
        printf("PL Conf JSON schema file is not a valid JSON\n");
        return 1;
      }
       else
       {
        //fprintf(stdout,"[%s::%d]-->PL Conf JSON schema file is a valid JSON\n",__FILE__,__LINE__);
       }
       if(m_confDataDoc.HasParseError())
       {
        //fprintf(stderr, "\nError(offset %u): %s\n", (unsigned)m_confDataDoc.GetErrorOffset(),GetParseError_En(m_confDataDoc.GetParseError()));
        printf("PL conf data JSON file is not a valid JSON\n");
        return 1;
       }
       else
       {
        //fprintf(stdout,"[%s::%d]-->JSON conf data : %s file is a valid JSON\n",__FILE__,__LINE__,fa_plJsonDataFile);
       }



       rapidjson::SchemaDocument l_schemaDocument(l_schemaDoc);

       rapidjson::SchemaValidator l_SchemaValidator(l_schemaDocument);

       // Now validating the Json data file against the json schema file
       if(m_confDataDoc.Accept(l_SchemaValidator))
       {
        printf("[%s::%s::%d] --> Data Parsing success against schema \n",__FILE__,__func__,__LINE__);
        m_originHost = std::string(m_confDataDoc["StackConf"]["originHost"].GetString());
        //fprintf(stdout,"[%s:%d]--> originHost  %s\n", __FILE__,__LINE__,m_originHost.c_str());

        m_originRealm = std::string(m_confDataDoc["StackConf"]["originRealm"].GetString());
        //fprintf(stdout,"[%s:%d]--> originRealm  %s\n", __FILE__,__LINE__,m_originRealm.c_str());

        //Fetching transport
        const rapidjson::Value& l_addressesArray = m_confDataDoc["StackConf"]["transport"]["addresses"];
        for (rapidjson::SizeType l_itr=0;l_itr<l_addressesArray.Size() ; l_itr++)
        {
         addresses l_addresses;
         const rapidjson::Value& l_ip_addressArray = l_addressesArray[l_itr]["ip_addresses"];
         for(rapidjson::SizeType l_itrIpAddr =0;l_itrIpAddr<l_ip_addressArray.Size(); l_itrIpAddr++)
         {
          std::string l_ipAddr=l_ip_addressArray[l_itrIpAddr].GetString();
          l_addresses.ip_address_list.push_back(l_ipAddr);
          //fprintf(stdout,"[%s:%d]--> ip_address  %s\n", __FILE__,__LINE__,l_ipAddr.c_str());
         }

         l_addresses.port = l_addressesArray[l_itr]["port"].GetInt();
         //fprintf(stdout,"[%s:%d]--> port  %d\n", __FILE__,__LINE__,l_addresses.port);

         std::string l_transportType=l_addressesArray[l_itr]["transport_protocol"].GetString();
         //fprintf(stdout,"[%s:%d]--> transport_protocol %s\n", __FILE__,__LINE__,l_transportType.c_str());
         if(!l_transportType.compare(std::string("tcp")))
          l_addresses.transport_protocol = DiaConfig::protocol::tcp_only;
         else if(!l_transportType.compare(std::string("sctp")))
          l_addresses.transport_protocol = DiaConfig::protocol::sctp_only;
         else if(!l_transportType.compare(std::string("both")))
          l_addresses.transport_protocol = DiaConfig::protocol::both;
         else
         {
          //fprintf(stdout,"[%s:%d]--> Error in transport_protocol .. setting tcp_only\n", __FILE__,__LINE__);
          l_addresses.transport_protocol = DiaConfig::protocol::tcp_only;
         }

         ip_addresses.push_back(l_addresses);

        }

        std::string l_mode(m_confDataDoc["StackConf"]["mode"].GetString());
        //fprintf(stdout,"[%s:%d]--> mode  %s\n", __FILE__,__LINE__,l_mode.c_str());
        if(!l_mode.compare(std::string("responder_mode")))
         m_mode = DiaConfig::mode::responder_mode;
        else if(!l_mode.compare(std::string("initiator_mode")))
         m_mode = DiaConfig::mode::initiator_mode;
        else
        {
         m_mode = DiaConfig::mode::responder_mode;
        }

        m_haEnabled=m_confDataDoc["StackConf"]["HAEnabled"].GetBool();
        //fprintf(stdout,"[%s:%d]--> HA Enabled  %d\n", __FILE__,__LINE__, m_haEnabled);

        std::string l_plStateManagement(m_confDataDoc["StackConf"]["plStateManagement"].GetString());
        //fprintf(stdout,"[%s:%d]--> plStateManagement  %s\n", __FILE__,__LINE__,l_plStateManagement.c_str());
        if(!l_plStateManagement.compare(std::string("state_aware")))
         state_mgmt = DiaConfig::pl_state_management::state_aware;
        else if(!l_plStateManagement.compare(std::string("no_state_aware")))
         state_mgmt = DiaConfig::pl_state_management::no_state_aware;
        else
        {
         //fprintf(stdout,"[%s:%d]--> Error in plStateManagement ..setting no_state_aware", __FILE__,__LINE__);
         state_mgmt = DiaConfig::pl_state_management::no_state_aware;
        }

        pl_end_points=m_confDataDoc["StackConf"]["plEndPoint"].GetString();
        //fprintf(stdout,"[%s:%d]--> PL Endpoint:%s\n", __FILE__,__LINE__,pl_end_points.c_str());

        m_productName = m_confDataDoc["StackConf"]["productName"].GetString();;
        //fprintf(stdout,"[%s:%d]--> productName  %s\n", __FILE__,__LINE__,m_productName.c_str());
        m_isProvingOn =m_confDataDoc["StackConf"]["isProvingOn"].GetBool();
        //fprintf(stdout,"[%s:%d]--> isProvingOn  %d\n", __FILE__,__LINE__,m_isProvingOn);

        m_provingNoTrafficTimeOut = m_confDataDoc["StackConf"]["provingNoTrafficTimeOut"].GetInt();
        //fprintf(stdout,"[%s:%d]--> provingNoTrafficTimeOut  %d\n", __FILE__,__LINE__,m_provingNoTrafficTimeOut);

        m_provingDWRretryCount = m_confDataDoc["StackConf"]["provingDWRRetryCount"].GetInt();
        //fprintf(stdout,"[%s:%d]--> provingDWRRetryCount  %d\n", __FILE__,__LINE__,m_provingDWRretryCount);
        m_provingDWAtimeOut = m_confDataDoc["StackConf"]["provingDWATimeout"].GetInt();
        //fprintf(stdout,"[%s:%d]--> provingDWAtimeOut  %d\n", __FILE__,__LINE__,m_provingDWAtimeOut);
        m_healthCheckNoTrafficTimeOut = m_confDataDoc["StackConf"]["healthCheckNoTrafficTimeOut"].GetInt();;
        //fprintf(stdout,"[%s:%d]--> m_healthCheckNoTrafficTimeOut  %d\n", __FILE__,__LINE__,m_healthCheckNoTrafficTimeOut);
        m_healthCheckDWAtimeOut = m_confDataDoc["StackConf"]["healthCheckDWATimeout"].GetInt();
        //fprintf(stdout,"[%s:%d]--> healthCheckDWAtimeOut  %d\n", __FILE__,__LINE__,m_healthCheckDWAtimeOut);
                             
        m_readWritePollingTimeOut =  m_confDataDoc["StackConf"]["readWritePollingTimeOut"].GetInt();
        //fprintf(stdout,"[%s:%d]--> readWritePollingTimeOut  %d\n", __FILE__,__LINE__,m_readWritePollingTimeOut);

        m_peerConnectionRetryTimer= m_confDataDoc["StackConf"]["peerConnectionRetryTimer"].GetInt();
        //fprintf(stdout,"[%s:%d]--> peerConnectionRetryTimer  %d\n", __FILE__,__LINE__,m_peerConnectionRetryTimer);

        m_vendorId = m_confDataDoc["StackConf"]["vendorID"].GetInt();
        //fprintf(stdout,"[%s:%d]--> vendorID  %d\n", __FILE__,__LINE__,m_vendorId);

        const rapidjson::Value& appIdArray=m_confDataDoc["StackConf"]["applicationIds"];
        for(rapidjson::SizeType i=0;i<appIdArray.Size();i++)
        {
         uint32_t l_appId=appIdArray[i].GetInt();
         //fprintf(stdout,"[%s:%d]--> applicationId  %d\n", __FILE__,__LINE__,l_appId);
         m_applicationIds.insert(l_appId);
        }

        m_firmWareRevision =  m_confDataDoc["StackConf"]["FirmWareRev"].GetInt();
        //fprintf(stdout,"[%s:%d]--> firmWareRevision  %d\n", __FILE__,__LINE__,m_firmWareRevision);

        m_dictionaryPath = m_confDataDoc["StackConf"]["dictionaryPath"].GetString();
        //fprintf(stdout,"[%s:%d]--> m_dictionaryPath  %s\n", __FILE__,__LINE__,m_dictionaryPath.c_str());

         m_logFilePath = m_confDataDoc["StackConf"]["logFilePath"].GetString();
                //fprintf(stdout,"[%s:%d]--> m_logFilePath %s\n", __FILE__,__LINE__,m_logFilePath.c_str());

        m_maxLogFileSize= m_confDataDoc["StackConf"]["maxLogFileSize"].GetUint64();
        //fprintf(stdout,"[%s:%d]--> m_maxLogFileSize %lu\n", __FILE__,__LINE__,m_maxLogFileSize);

        m_logLevel=m_confDataDoc["StackConf"]["logLevel"].GetUint();
        //fprintf(stdout,"[%s:%d]--> m_logLevel %u\n", __FILE__,__LINE__,m_logLevel);
        //
        m_flushSize=m_confDataDoc["StackConf"]["flushSize"].GetUint();
        //fprintf(stdout,"[%s:%d]--> m_flushSize %lu\n", __FILE__,__LINE__,m_flushSize);

        m_flushTime=m_confDataDoc["StackConf"]["flushTime"].GetUint();
        //fprintf(stdout,"[%s:%d]--> m_flushTime %u\n", __FILE__,__LINE__,m_flushTime);
        const rapidjson::Value& l_peerArray=m_confDataDoc["StackConf"]["memPoolPageListPL"];
        for(rapidjson::SizeType i=0;i<l_peerArray.Size();i++)
        {
            //fprintf(stdout,"[%s:%d]--> memPoolPagePL data Entry Number: %d .. \n", __FILE__,__LINE__,i+1);
            std::pair<uint32_t, uint32_t>  l_memPoolPagePL;
            l_memPoolPagePL.first= l_peerArray[i]["memPoolPages"].GetInt();
            //fprintf(stdout,"[%s:%d]--> memPoolPages %d \n", __FILE__,__LINE__,l_memPoolPagePL.first);

            l_memPoolPagePL.second = l_peerArray[i]["memPoolPageSize"].GetInt();
            //fprintf(stdout,"[%s:%d]--> memPoolPageSize %d \n", __FILE__,__LINE__,l_memPoolPagePL.second);

            m_memPoolPageList.push_back(l_memPoolPagePL);
        }
        m_httpServerPort =  m_confDataDoc["StackConf"]["httpServerPort"].GetInt();
        //fprintf(stdout,"[%s:%d]--> httpServerPort  %d\n", __FILE__,__LINE__,m_httpServerPort);

        m_httpServerIPAddr=m_confDataDoc["StackConf"]["httpServerIPAddr"].GetString();
        //fprintf(stdout,"[%s:%d]--> httpServerIPAddr  %s\n", __FILE__,__LINE__,m_httpServerIPAddr.c_str());

       }
       else
       {
        //fprintf(stdout,"[%s::%s::%d] --> Json Data Parsing failed against json schema \n",__FILE__,__func__,__LINE__);
        return 1;
       }


       return 0;
      }
      catch(...)
      {
       //fprintf(stdout,"[%s:%d]--> Error in Parsing the Json file ..\n", __FILE__,__LINE__);
       return 1;
      }
     }

    }
   }
  }
  
  
  ****JioJsonSchemas.hpp*****
  
  #ifndef __JIO_JSON_SCHEMAS__
#define __JIO_JSON_SCHEMAS__

// PL Configuration Json Schema
const char g_plJsonSchema[]="{\
  \"definitions\": {\
    \"AddressesDef\": {\
      \"title\": \"PL trasport configuration schema\",\
      \"description\": \"JSON schema for PL transport configuration\",\
      \"type\": \"array\",\
      \"items\": {\
        \"type\": \"object\",\
        \"properties\": {\
          \"ip_addresses\": {\
            \"type\": \"array\",\
            \"items\": {\
              \"type\": \"string\"\
            }\
          },\
          \"port\": {\
            \"type\": \"number\"\
          },\
          \"transport_protocol\": {\
            \"type\": \"string\",\
            \"enum\": [\
              \"sctp\",\
              \"tcp\",\
              \"both\"\
            ]\
          }\
        },\
        \"required\": [\
          \"ip_addresses\",\
          \"port\",\
          \"transport_protocol\"\
        ]\
      }\
    },\

    \"PeerDef\": {\
      \"title\": \"Peer Table Entry\",\
      \"description\": \"JSON schema for an entry in Peer table\",\
      \"type\": \"array\",\
      \"items\": {\
        \"properties\": {\
          \"peerHost\": {\
            \"type\": \"string\"\
          },\
          \"peerRealm\": {\
            \"type\": \"string\"\
          },\
          \"peerIpAddr\": {\
            \"type\": \"string\"\
          },\
          \"peerPort\": {\
            \"type\": \"number\"\
          },\
          \"transportType\": {\
            \"type\": \"string\",\
            \"enum\": [\
              \"sctp\",\
              \"tcp\",\
              \"both\"\
            ]\
          },\
          \"adminState\": {\
            \"type\": \"string\",\
            \"enum\": [\
              \"enabled\",\
              \"disabled\"\
            ]\
          }\
        },\
        \"required\": [\
          \"peerHost\",\
          \"peerRealm\",\
          \"peerIpAddr\",\
          \"peerPort\",\
          \"transportType\",\
          \"adminState\"\
        ]\
      }\
    },\
    \"RouteDef\": {\
      \"title\": \"Route Table Entry\",\
      \"description\": \"JSON schema for an entry in route table\",\
      \"type\": \"array\",\
      \"items\": {\
        \"properties\": {\
          \"Realm\": {\
            \"type\": \"string\"\
          },\
          \"appId\": {\
            \"type\": \"number\"\
          },\
          \"peerHost\": {\
            \"type\": \"string\"\
          },\
          \"priority\": {\
            \"type\": \"number\"\
          },\
          \"adminState\": {\
            \"type\": \"string\",\
            \"enum\": [\
              \"enabled\",\
              \"disabled\"\
            ]\
          }\
        },\
        \"required\": [\
          \"Realm\",\
          \"appId\",\
          \"peerHost\",\
          \"priority\",\
          \"adminState\"\
        ]\
      }\
    },\
    \"memPoolPageListPLDef\": {\
      \"title\": \"Memory Pool Page List PL\",\
      \"description\": \"JSON schema for memory pool page list of PL\",\
      \"type\": \"array\",\
      \"items\": {\
        \"properties\": {\
          \"memPoolPages\": {\
            \"type\": \"number\"\
          },\
          \"memPoolPageSize\": {\
            \"type\": \"number\"\
          }\
        },\
        \"required\": [\
          \"memPoolPages\",\
          \"memPoolPageSize\"\
        ]\
      }\
  }\
},\
  \"$schema\": \"http://json-schema.org/draft-04/schema#\",\
  \"title\": \"PL configuration schema\",\
  \"description\": \"JSON schema for PL configuration\",\
  \"type\": \"object\",\
  \"properties\": {\
    \"StackConf\": {\
      \"title\": \"Stack Configuration\",\
      \"description\": \"JSON schema of stack configuration\",\
      \"type\": \"object\",\
      \"properties\": {\
        \"originHost\": {\
          \"type\": \"string\"\
        },\
        \"originRealm\": {\
          \"type\": \"string\"\
        },\
        \"transport\": {\
          \"type\": \"object\",\
          \"properties\": {\
            \"addresses\": {\
              \"$ref\": \"#/definitions/AddressesDef\"\
            }\
          }\
        },\
        \"mode\": {\
          \"type\": \"string\",\
          \"enum\": [\
            \"responder_mode\",\
            \"initiator_mode\"\
          ]\
        },\
 \"HAEnabled\": {\
          \"type\": \"boolean\"\
        },\
        \"plStateManagement\": {\
          \"type\": \"string\",\
          \"enum\": [\
            \"no_state_aware\",\
            \"state_aware\"\
          ]\
        },\
        \"plEndPoint\": {\
          \"type\": \"string\"\
        },\
        \"productName\": {\
          \"type\": \"string\"\
        },\
        \"isProvingOn\": {\
          \"type\": \"boolean\"\
        },\
        \"provingNoTrafficTimeOut\": {\
          \"type\": \"number\"\
        },\
        \"provingDWRRetryCount\": {\
          \"type\": \"number\"\
        },\
        \"provingDWATimeout\": {\
          \"type\": \"number\"\
        },\
        \"healthCheckNoTrafficTimeOut\": {\
          \"type\": \"number\"\
        },\
        \"healthCheckDWRRetryCount\": {\
          \"type\": \"number\"\
        },\
        \"healthCheckDWATimeout\": {\
          \"type\": \"number\"\
        },\
        \"readWritePollingTimeOut\": {\
          \"type\": \"number\"\
        },\
  \"peerConnectionRetryTimer\": {\
            \"type\": \"number\"\
         },\
        \"vendorID\": {\
          \"type\": \"number\"\
        },\
        \"applicationIds\": {\
          \"type\": \"array\",\
          \"items\": {\
            \"type\": \"number\"\
          }\
        },\
        \"FirmWareRev\": {\
          \"type\": \"number\"\
        },\
 \"dictionaryPath\": {\
          \"type\": \"string\"\
        },\
 \"logFilePath\": {\
          \"type\": \"string\"\
        },\
 \"maxLogFileSize\": {\
          \"type\": \"number\"\
        },\
 \"logLevel\": {\
          \"type\": \"number\"\
        },\
 \"flushSize\": {\
          \"type\": \"number\"\
        },\
 \"flushTime\": {\
          \"type\": \"number\"\
        },\
    \"memPoolPageListPL\": {\
      \"$ref\": \"#/definitions/memPoolPageListPLDef\"\
    },\
        \"httpServerPort\": {\
          \"type\": \"number\"\
        },\
 \"httpServerIPAddr\": {\
          \"type\": \"string\"\
        }\
  },\
      \"required\": [\
        \"originHost\",\
        \"originRealm\",\
        \"transport\",\
        \"mode\",\
        \"HAEnabled\",\
        \"plStateManagement\",\
        \"plEndPoint\",\
        \"productName\",\
        \"isProvingOn\",\
        \"provingNoTrafficTimeOut\",\
        \"provingDWRRetryCount\",\
        \"provingDWATimeout\",\
        \"healthCheckNoTrafficTimeOut\",\
        \"healthCheckDWRRetryCount\",\
        \"healthCheckDWATimeout\",\
        \"readWritePollingTimeOut\",\
        \"peerConnectionRetryTimer\",\
        \"vendorID\",\
        \"applicationIds\",\
        \"FirmWareRev\",\
    \"dictionaryPath\",\
    \"logFilePath\",\
    \"maxLogFileSize\",\
    \"logLevel\",\
    \"flushSize\",\
    \"flushTime\",\
        \"memPoolPageListPL\",\
        \"httpServerPort\",\
        \"httpServerIPAddr\"\
      ]\
    },\
    \"PeerTable\": {\
      \"$ref\": \"#/definitions/PeerDef\"\
    },\
    \"RoutingTable\": {\
      \"$ref\": \"#/definitions/RouteDef\"\
    }\
  },\
  \"required\": [\
    \"StackConf\",\
    \"PeerTable\",\
    \"RoutingTable\"\
  ]\
}";

//Peer Addition JSON Schema
const char g_peerAddSchema[]="{\
        \"$schema\": \"http://json-schema.org/draft-04/schema#\",\
        \"title\": \"Peer addition schema\",\
        \"description\": \"JSON schema for validating insertion into Peer Table\",\
        \"type\": \"object\",\
        \"properties\": {\
                \"peerHost\": {\
                        \"type\": \"string\"\
                },\
                \"peerRealm\": {\
                        \"type\": \"string\"\
                },\
                \"peerIpAddr\": {\
                        \"type\": \"string\"\
                },\
                \"peerPort\": {\
                        \"type\": \"number\"\
                },\
                \"transportType\": {\
                        \"type\": \"string\",\
                        \"enum\": [\
                                \"sctp\",\
                        \"tcp\",\
                        \"both\"\
                                ]\
                },\
                \"adminState\": {\
                        \"type\": \"string\",\
                        \"enum\": [\
                                \"enabled\",\
                        \"disabled\"\
                                ]\
                        }\
        },\
        \"required\": [\
                \"peerHost\",\
        \"peerRealm\",\
        \"peerIpAddr\",\
        \"peerPort\",\
        \"transportType\",\
 \"adminState\"\
                ]\
}";
//Peer Updation JSON Schema
const char g_peerUpdateSchema[]="{\
        \"$schema\": \"http://json-schema.org/draft-04/schema#\",\
        \"title\": \"Peer Update\",\
        \"description\": \"JSON schema for updation of peer entry in peer table\",\
        \"type\": \"object\",\
        \"properties\": {\
                \"peerHost\": {\
                        \"type\": \"string\"\
                },\
                \"peerRealm\": {\
                        \"type\": \"string\"\
                },\
                \"peerIpAddr\": {\
                        \"type\": \"string\"\
                },\
                \"peerPort\": {\
                        \"type\": \"number\"\
                },\
                \"transportType\": {\
                        \"type\": \"string\",\
                        \"enum\": [\
                                \"sctp\",\
                        \"tcp\",\
                        \"both\"\
                                ]\
                },\
                \"adminState\": {\
                        \"type\": \"string\",\
                        \"enum\": [\
                                \"enabled\",\
                        \"disabled\"\
                                ]\
                        }\
                },\
                \"anyOf\": [\
                                {\"required\": [\"peerRealm\"]},\
                                {\"required\": [\"peerIpAddr\"]},\
                                {\"required\": [\"peerPort\"]},\
                                {\"required\": [\"transportType\"]},\
                                {\"required\": [\"adminState\"]}\
                        ],\
                \"required\": [\"peerHost\"]\
}";

//Peer Deletion JSON Schema
const char g_peerDeleteSchema[]="{\
        \"$schema\": \"http://json-schema.org/draft-04/schema#\",\
        \"title\": \"Peer deletion schema\",\
        \"description\": \"JSON schema for validating deletion of an entry from peer table\",\
        \"type\": \"object\",\
        \"properties\": {\
                \"peerHost\": {\
                        \"type\": \"string\"\
                }\
        },\
        \"required\": [\
                \"peerHost\"\
                ]\
}";

//Get Peer Counters JSON Schema
const char g_getPeerCountersSchema[]="{\
        \"$schema\": \"http://json-schema.org/draft-04/schema#\",\
        \"title\": \"Peer counters retrieval schema\",\
        \"description\": \"JSON schema for validating retrieval of counters of an entry from peer table\",\
        \"type\": \"object\",\
        \"properties\": {\
                \"peerHost\": {\
                        \"type\": \"string\"\
                }\
        },\
        \"required\": [\
                \"peerHost\"\
                ]\
}";
//Reset Peer Counters JSON Schema
const char g_resetPeerCountersSchema[]="{\
        \"$schema\": \"http://json-schema.org/draft-04/schema#\",\
        \"title\": \"Peer counters reset schema\",\
        \"description\": \"JSON schema for validating resettting of counters of an entry in  peer table\",\
        \"type\": \"object\",\
        \"properties\": {\
                \"peerHost\": {\
                        \"type\": \"string\"\
                }\
        },\
        \"required\": [\
                \"peerHost\"\
                ]\
}";

//Peer Link Reset JSON Schema
const char g_peerLinkResetSchema[]="{\
        \"$schema\": \"http://json-schema.org/draft-04/schema#\",\
        \"title\": \"Peer Link Reset schema\",\
        \"description\": \"JSON schema for validating resetting of peer's transport link\",\
        \"type\": \"object\",\
        \"properties\": {\
                \"peerHost\": {\
                        \"type\": \"string\"\
                }\
        },\
        \"required\": [\
                \"peerHost\"\
                ]\
}";

//Route Addition JSON Schema
const char g_routeAddSchema[]="{\
        \"$schema\": \"http://json-schema.org/draft-04/schema#\",\
        \"title\": \"Route addition schema\",\
        \"description\": \"JSON schema for validating insertion into Route Table\",\
        \"type\": \"object\",\
        \"properties\": {\
                \"Realm\": {\
                        \"type\": \"string\"\
                },\
                \"appId\": {\
                        \"type\": \"number\"\
                },\
                \"peerHost\": {\
                        \"type\": \"string\"\
                },\
                \"priority\": {\
                        \"type\": \"number\"\
                },\
                \"adminState\": {\
                        \"type\": \"string\",\
                        \"enum\": [\
                                \"enabled\",\
                        \"disabled\"\
                                ]\
                }\
        },\
        \"required\": [\
                \"Realm\",\
        \"appId\",\
        \"peerHost\",\
        \"priority\",\
        \"adminState\"\
        ]\
}";

//Route Updation JSON Schema
const char g_routeUpdateSchema[]="{\
        \"$schema\": \"http://json-schema.org/draft-04/schema#\",\
        \"title\": \"Route updation schema\",\
        \"description\": \"JSON schema for validating updation of an entry into Route Table\",\
        \"type\": \"object\",\
        \"properties\": {\
                \"Realm\": {\
                        \"type\": \"string\"\
                },\
                \"appId\": {\
                        \"type\": \"number\"\
                },\
                \"peerHost\": {\
                        \"type\": \"string\"\
                },\
                \"priority\": {\
                        \"type\": \"number\"\
                },\
                \"adminState\": {\
                        \"type\": \"string\",\
                        \"enum\": [\
                                \"enabled\",\
                        \"disabled\"\
                                ]\
                }\
        },\
        \"anyOf\": [\
                        {\"required\":[\"priority\"]},\
                        {\"required\":[\"adminState\"]}\
                ],\
        \"required\": [\"Realm\",\"appId\",\"peerHost\"]\
}";

//Route Deletion JSON Schema
const char g_routeDeleteSchema[]="{\
        \"$schema\": \"http://json-schema.org/draft-04/schema#\",\
        \"title\": \"Route deletion schema\",\
        \"description\": \"JSON schema for validating deletion of an entry into Route Table\",\
        \"type\": \"object\",\
        \"properties\": {\
                \"Realm\": {\
                        \"type\": \"string\"\
                },\
                \"appId\": {\
                        \"type\": \"number\"\
                },\
                \"peerHost\": {\
                        \"type\": \"string\"\
                }\
        },\
        \"required\": [\
                \"Realm\",\
                \"appId\",\
                \"peerHost\"\
        ]\
}";

//Peer Info Display JSON Schema
const char g_viewPeerInfoSchema[]="{\
        \"$schema\": \"http://json-schema.org/draft-04/schema#\",\
        \"title\": \"Peer Info Display schema\",\
        \"description\": \"JSON schema for validating display of info of a single peer\",\
        \"type\": \"object\",\
        \"properties\": {\
                \"peerHost\": {\
                        \"type\": \"string\"\
                }\
        },\
        \"required\": [\
                \"peerHost\"\
        ]\
}";

//Route Info Display JSON Schema
const char g_viewRouteInfoSchema[]="{\
        \"$schema\": \"http://json-schema.org/draft-04/schema#\",\
        \"title\": \"Route Info Display schema\",\
        \"description\": \"JSON schema for validating display of info of a single route\",\
        \"type\": \"object\",\
        \"properties\": {\
                \"Realm\": {\
                        \"type\": \"string\"\
                },\
                \"appId\": {\
                        \"type\": \"number\"\
                }\
        },\
        \"required\": [\
                \"Realm\",\
		\"appId\"\
        ]\
}";

//Set Log Level JSON Schema
const char g_setLogLevelSchema[]="{\
        \"$schema\": \"http://json-schema.org/draft-04/schema#\",\
        \"title\": \"Set log level schema\",\
        \"description\": \"JSON schema for validating request for setting log level\",\
        \"type\": \"object\",\
        \"properties\": {\
                \"logLevel\": {\
                        \"type\": \"number\"\
                }\
        },\
        \"required\": [\
                \"logLevel\"\
        ]\
}";
#endif//__JIO_JSON_SCHEMAS__


******Stack_Event_Schema.json********
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "title": "Event schema",
  "description": "This is json schema for fault/events",
  "type": "object",
  "properties": {
    "EventType": {
      "description": "Event type name",
      "type": "string",
      "enum": [
        "PEER_UP",
        "PEER_DOWN",
	"PL_UP",
	"PL_DOWN",
	"UL_UP",
	"UL_DOWN",
	"COMM_ERROR",
	"INIT_ERROR",
        "PEER_HEALTH_CHK_MISS",
        "RESOURCE_THRES_BREACH"
      ]
    },
    "OriginatingObject": {
      "description": "Event Orginating object name",
      "type": "string",
      "maxLength": 30
    },
    "TimeStamp": {
      "description": "Timestamp of event",
      "type": "string",
      "maxLength": 30
    },
    "Description": {
      "description": "Detail of the event occured",
      "type": "string",
      "maxLength": 50
    }
  },
  "required": [
    "EventType",
    "OriginatingObject",
    "TimeStamp",
    "Description"
  ]
}


**********Stack_UL_configuration_Data.json***********

{
			"numOfWorkerThreads": 6,
			"UlapplicationIds": [4],
			"provideEndPoint": "/tmp/diameter.endpoint5500",
			"ulName": "OCS5500",
			"dictionaryPath": "data/Dictionary.xml",
				"logFilePath": "/tmp/",
				"maxLogFileSize": 10240000,
				"logLevel":1,
			"isValidationOn": true,
				"memPoolPageList": [
				{
								"memPoolPages" : 300000,
								"memPoolPageSize" : 1000
				},
				{
								"memPoolPages" : 400000,
								"memPoolPageSize" : 3000
				},
				{
								"memPoolPages" : 500000,
								"memPoolPageSize" : 500
				}
				],
			"sessionPoolSize": 100000
}

                                                      




